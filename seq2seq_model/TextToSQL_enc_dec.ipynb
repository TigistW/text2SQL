{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPSakCdZjaFH"
      },
      "source": [
        "# Environment setup and imports\n",
        "Import core libraries, set deterministic seeds, and prepare Keras/TensorFlow utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywzq_QdJjaFK",
        "outputId": "40a7de41-231a-4166-9234-572e21042b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.46.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.3.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m758.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.46.3-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.12.19 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.5 wheel-0.46.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Reproducible randomness\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1LTe08UjjaFT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, LSTM, Embedding, InputLayer, Bidirectional, TimeDistributed, Input, Concatenate, Reshape, Lambda, Flatten, RepeatVector, Dot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhAiUVvCjaFW"
      },
      "source": [
        "# Loading dataset\n",
        "Read parallel NL (natural language) and SQL files into memory and perform basic sanity checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGH4_xq7jaFY",
        "outputId": "c120aab8-8fea-4633-d1bb-cce3c15d275f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 37113 train NL lines and 37113 train SQL lines.\n",
            "Loaded 448 test NL lines and 448 test SQL lines.\n"
          ]
        }
      ],
      "source": [
        "# Load parallel NL (natural language) and SQL files. Strip empty lines and trailing whitespace.\n",
        "\n",
        "def load_lines(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f.read().splitlines() if line.strip()]\n",
        "\n",
        "ftrain_x = load_lines('/content/new_train.nl')\n",
        "ftrain_y = load_lines('/content/new_train.sql')\n",
        "ftest_x = load_lines('/content/test.nl')\n",
        "ftest_y = load_lines('/content/test.sql')\n",
        "\n",
        "print(f\"Loaded {len(ftrain_x)} train NL lines and {len(ftrain_y)} train SQL lines.\")\n",
        "print(f\"Loaded {len(ftest_x)} test NL lines and {len(ftest_y)} test SQL lines.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eocYrxIsjaFf",
        "outputId": "65a0f5fc-c57d-45b5-a420-8a9fbee3507d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered train pairs: 9401\n",
            "Filtered test pairs: 186\n"
          ]
        }
      ],
      "source": [
        "# Keep only pairs where SQL length is <= max_sql_len to avoid extremely long targets.\n",
        "max_sql_len = 50\n",
        "\n",
        "train_x, train_y, test_x, test_y = [], [], [], []\n",
        "\n",
        "for x, y in zip(ftrain_x, ftrain_y):\n",
        "    if len(y.split()) <= max_sql_len:\n",
        "        train_x.append(x)\n",
        "        train_y.append(y)\n",
        "\n",
        "for x, y in zip(ftest_x, ftest_y):\n",
        "    if len(y.split()) <= max_sql_len:\n",
        "        test_x.append(x)\n",
        "        test_y.append(y)\n",
        "\n",
        "print(f\"Filtered train pairs: {len(train_x)}\")\n",
        "print(f\"Filtered test pairs: {len(test_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jagKKOcWjaFk"
      },
      "source": [
        "# Creating Index for NL Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56TzcTRjaFm",
        "outputId": "8558b51f-ab16-451a-9f28-12f74623c3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NL vocab size (including special tokens): 741\n"
          ]
        }
      ],
      "source": [
        "# Build deterministic NL vocabulary (sorted) and reserve <PAD> and <UNK> as indices 0 and 1.\n",
        "train_corpus_nl = sorted({word for sentence in train_x for word in sentence.split()})\n",
        "\n",
        "# Reserve special tokens: <PAD>=0, <UNK>=1\n",
        "idx2word_nl = {i: token for i, token in enumerate(['<PAD>', '<UNK>'])}\n",
        "word2idx_nl = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "for i, word in enumerate(train_corpus_nl, start=2):\n",
        "    word2idx_nl[word] = i\n",
        "    idx2word_nl[i] = word\n",
        "\n",
        "len_x = len(idx2word_nl)\n",
        "print(f\"NL vocab size (including special tokens): {len_x}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gwtY7_EPjaFo"
      },
      "outputs": [],
      "source": [
        "# Tokenize NL training sentences and map tokens to indices (use <UNK> when missing).\n",
        "words_x = [[w for w in s.split()] for s in train_x]\n",
        "words_x_idx = [[word2idx_nl.get(w, word2idx_nl['<UNK>']) for w in sent] for sent in words_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtPQZNejaFs"
      },
      "source": [
        "# Creating vocabulary for SQL targets\n",
        "Build deterministic token-index mappings for the SQL (target) side, reserving special tokens (<PAD>, <UNK>, <SOS>, <EOS>)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oj4uo_8jaFt",
        "outputId": "cd216068-3dd1-4c71-d229-5c3cc5d7c734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL vocab size (including special tokens): 397\n"
          ]
        }
      ],
      "source": [
        "# Build SQL vocabulary with deterministic ordering and reserve special tokens: <PAD>, <UNK>, <SOS>, <EOS>\n",
        "train_corpus_sql = sorted({word for sentence in train_y for word in sentence.split()})\n",
        "\n",
        "special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
        "word2idx_sql = {tok: i for i, tok in enumerate(special_tokens)}\n",
        "idx2word_sql = {i: tok for i, tok in enumerate(special_tokens)}\n",
        "\n",
        "for i, word in enumerate(train_corpus_sql, start=len(special_tokens)):\n",
        "    word2idx_sql[word] = i\n",
        "    idx2word_sql[i] = word\n",
        "\n",
        "len_y = len(idx2word_sql)\n",
        "print(f\"SQL vocab size (including special tokens): {len_y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q-1eouV2jaFx"
      },
      "outputs": [],
      "source": [
        "# Decoder inputs: prepend <SOS>. Targets: append <EOS>.\n",
        "words_y = [['<SOS>'] + s.split() + ['<EOS>'] for s in train_y]\n",
        "words_y_idx = [[word2idx_sql.get(w, word2idx_sql['<UNK>']) for w in sent] for sent in words_y]\n",
        "\n",
        "words_target = [s.split() + ['<EOS>'] for s in train_y]\n",
        "words_target_idx = [[word2idx_sql.get(w, word2idx_sql['<UNK>']) for w in sent] for sent in words_target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAknipUGjaF3",
        "outputId": "2a88decd-c7ad-4799-824b-f8708c7a1c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_x=23, max_y=52, len_x=741, len_y=397\n"
          ]
        }
      ],
      "source": [
        "# Compute maximum sequence lengths for padding and print a short summary\n",
        "max_x = max(len(sent) for sent in words_x_idx)\n",
        "max_y = max(len(sent) for sent in words_y_idx)\n",
        "\n",
        "print(f\"max_x={max_x}, max_y={max_y}, len_x={len_x}, len_y={len_y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-271HqpygPTV"
      },
      "outputs": [],
      "source": [
        "# (Formatting cell) Small placeholder to separate preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-jHOH7avjaF7"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to max lengths using the <PAD> token index\n",
        "words_x_idx_pad = [sent + [word2idx_nl['<PAD>']] * (max_x - len(sent)) for sent in words_x_idx]\n",
        "words_y_idx_pad = [sent + [word2idx_sql['<PAD>']] * (max_y - len(sent)) for sent in words_y_idx]\n",
        "words_target_idx_pad = [sent + [word2idx_sql['<PAD>']] * (max_y - len(sent)) for sent in words_target_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4egbHZ8ggCei"
      },
      "outputs": [],
      "source": [
        "# Convert padded integer sequences to one-hot vectors using `to_categorical` (vectorized and more efficient).\n",
        "words_x_idx_arr = to_categorical(np.array(words_x_idx_pad), num_classes=len_x)\n",
        "words_y_idx_arr = to_categorical(np.array(words_y_idx_pad), num_classes=len_y)\n",
        "words_target_idx_arr = to_categorical(np.array(words_target_idx_pad), num_classes=len_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-N2iiKZP9Sar"
      },
      "outputs": [],
      "source": [
        "# The previous manual loop-based one-hot encoding has been replaced by `to_categorical` above (faster and less memory-savvy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCoBBzvh_39b",
        "outputId": "9d0ee212-7b56-42cc-8b87-3ec502750991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot vector dimension for targets: 397\n"
          ]
        }
      ],
      "source": [
        "# Display the dimension of the one-hot vectors for targets\n",
        "print(\"One-hot vector dimension for targets:\", words_target_idx_arr.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm3MthW7jaF-",
        "outputId": "dcd70baa-cdc4-484e-f97f-d0c0ea83f7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted one-hot arrays shapes:\n",
            "words_x_idx_arr: (9401, 23, 741)\n",
            "words_y_idx_arr: (9401, 52, 397)\n",
            "words_target_idx_arr: (9401, 52, 397)\n"
          ]
        }
      ],
      "source": [
        "# Sanity-check: display shapes of the converted arrays\n",
        "print(\"Converted one-hot arrays shapes:\")\n",
        "print(\"words_x_idx_arr:\", words_x_idx_arr.shape)\n",
        "print(\"words_y_idx_arr:\", words_y_idx_arr.shape)\n",
        "print(\"words_target_idx_arr:\", words_target_idx_arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7BESBSNjaGE",
        "outputId": "5bf4c918-58f0-4398-fa5a-a0d49a024888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape check (encoder input, decoder input, target):\n",
            "(9401, 23, 741) (9401, 52, 397) (9401, 52, 397)\n"
          ]
        }
      ],
      "source": [
        "# Quick shape check for the main arrays\n",
        "print(\"Shape check (encoder input, decoder input, target):\")\n",
        "print(words_x_idx_arr.shape, words_y_idx_arr.shape, words_target_idx_arr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGtmkaLRjaGK"
      },
      "source": [
        "# Encoder model\n",
        "Build the encoder that consumes sequences of input one-hot vectors and returns final states (hidden and cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zcdoSQe9jaGL"
      },
      "outputs": [],
      "source": [
        "# Reference: earlier attempts at using Embedding for encoder. Kept for reference in case you want to switch from one-hot inputs to embeddings.\n",
        "# Example: enEmbed = Embedding(input_dim=len(idx2word_nl), output_dim=300)\n",
        "# enLSTM = LSTM(256, return_state=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BGbOCKE8Pc2W"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Encoder: processes a sequence of one-hot vectors (timesteps x len_x) and returns the final hidden and cell states.\n",
        "Input shape: (batch, Tx, len_x) where each timestep is a one-hot vector of size len_x.\n",
        "\"\"\"\n",
        "\n",
        "enInput = Input(shape=(None, len_x), name='encoder_input')\n",
        "enLSTM = LSTM(64, return_state=True, return_sequences=False, name='encoder_lstm')\n",
        "enOutput, enHiddenState, enCellState = enLSTM(enInput)\n",
        "enFinal = [enHiddenState, enCellState]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrFW7YdAjaGS",
        "outputId": "4b771d08-921c-4f43-9510-32d8ba08fbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder cell-state shape: (None, 64)\n"
          ]
        }
      ],
      "source": [
        "# Print encoder cell-state shape for quick verification\n",
        "print(\"Encoder cell-state shape:\", enCellState.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhoJpjJjaGZ"
      },
      "source": [
        "# Decoder model\n",
        "Configure the decoder to generate output sequences using initial states from the encoder and a softmax output over the target vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JD34XLNSjaGc"
      },
      "outputs": [],
      "source": [
        "# Reference: earlier decoder sketches using Embedding; kept as comments for quick experimentation.\n",
        "# If you move from one-hot inputs to embeddings, uncomment and adapt these.\n",
        "# deEmbed = Embedding(input_dim=len(idx2word_sql), output_dim=300)\n",
        "# deLSTM = LSTM(256, return_sequences=True, return_state=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAJOnJq3RMPc",
        "outputId": "da43d186-8085-4230-f1a8-f99a366d0f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (None, None, 397)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Decoder: consumes one-hot vectors per timestep with initial states from the encoder.\n",
        "Return sequences (outputs over all timesteps), then project to SQL vocabulary with a Dense+softmax.\n",
        "'''\n",
        "\n",
        "deInput = Input(shape=(None, len_y), name='decoder_input')\n",
        "deLSTM = LSTM(64, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "deOutput, deHiddenState, deCellState = deLSTM(deInput, initial_state=enFinal)\n",
        "deDense = Dense(len_y, activation='softmax', name='decoder_output_dense')\n",
        "deDenseOutput = deDense(deOutput)\n",
        "\n",
        "# Print decoder output shape (symbolic tensor)\n",
        "print('Decoder output shape:', deDenseOutput.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Tqwkv922jaGj",
        "outputId": "442147bf-7e42-44cf-f881-149e8821baf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"seq2seq_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m741\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m397\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),      │    \u001b[38;5;34m206,336\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m118,272\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),  │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output_den… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m397\u001b[0m) │     \u001b[38;5;34m25,805\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">741</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">397</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">206,336</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">118,272</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),  │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output_den… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">397</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,805</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m350,413\u001b[0m (1.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,413</span> (1.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m350,413\u001b[0m (1.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,413</span> (1.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Model(inputs=[enInput, deInput], outputs=[deDenseOutput], name='seq2seq_model')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9mqIed2jaGo",
        "outputId": "49491902-7e9a-4997-fc7e-6627b5f28c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 91ms/step - accuracy: 0.2583 - loss: 4.6718 - val_accuracy: 0.3879 - val_loss: 2.5067\n",
            "Epoch 2/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.4265 - loss: 2.3528 - val_accuracy: 0.5301 - val_loss: 2.0333\n",
            "Epoch 3/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.5624 - loss: 1.9156 - val_accuracy: 0.6121 - val_loss: 1.7016\n",
            "Epoch 4/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.6369 - loss: 1.5722 - val_accuracy: 0.7602 - val_loss: 1.3376\n",
            "Epoch 5/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8121 - loss: 1.1697 - val_accuracy: 0.8273 - val_loss: 0.9790\n",
            "Epoch 6/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8475 - loss: 0.8327 - val_accuracy: 0.8515 - val_loss: 0.7827\n",
            "Epoch 7/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.8771 - loss: 0.6565 - val_accuracy: 0.8692 - val_loss: 0.6718\n",
            "Epoch 8/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.8925 - loss: 0.5536 - val_accuracy: 0.8799 - val_loss: 0.5922\n",
            "Epoch 9/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9028 - loss: 0.4823 - val_accuracy: 0.8904 - val_loss: 0.5356\n",
            "Epoch 10/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.9125 - loss: 0.4281 - val_accuracy: 0.8960 - val_loss: 0.4902\n",
            "Epoch 11/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9186 - loss: 0.3849 - val_accuracy: 0.8998 - val_loss: 0.4540\n",
            "Epoch 12/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9226 - loss: 0.3516 - val_accuracy: 0.9034 - val_loss: 0.4273\n",
            "Epoch 13/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9271 - loss: 0.3230 - val_accuracy: 0.9063 - val_loss: 0.4114\n",
            "Epoch 14/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.9315 - loss: 0.3016 - val_accuracy: 0.9129 - val_loss: 0.3875\n",
            "Epoch 15/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9353 - loss: 0.2821 - val_accuracy: 0.9156 - val_loss: 0.3659\n",
            "Epoch 16/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9379 - loss: 0.2639 - val_accuracy: 0.9206 - val_loss: 0.3453\n",
            "Epoch 17/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9409 - loss: 0.2468 - val_accuracy: 0.9222 - val_loss: 0.3321\n",
            "Epoch 18/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.9434 - loss: 0.2334 - val_accuracy: 0.9243 - val_loss: 0.3202\n",
            "Epoch 19/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.9455 - loss: 0.2217 - val_accuracy: 0.9247 - val_loss: 0.3141\n",
            "Epoch 20/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.9470 - loss: 0.2116 - val_accuracy: 0.9264 - val_loss: 0.3040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fc033475700>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Compile and train the seq2seq model using a safer default learning rate and early stopping\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
        "optimizer = Adam(1e-3)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=[words_x_idx_arr, words_y_idx_arr], y=words_target_idx_arr,\n",
        "          batch_size=64, validation_split=0.2, epochs=20, verbose=1, shuffle=True, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB6TzQJ7tNwn"
      },
      "source": [
        "# Inference models\n",
        "Build lightweight encoder/decoder models for step-by-step decoding during inference (greedy decoding below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gFIUi4CKjaGt"
      },
      "outputs": [],
      "source": [
        "# Encoder inference model: returns final hidden and cell states for a given input sequence\n",
        "enInfModel = Model(inputs=enInput, outputs=enFinal, name='encoder_inference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdyI09x0TNjO",
        "outputId": "6eceea14-0315-4ce3-90c3-158842bcc507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder input symbolic shape: (None, None, 397)\n"
          ]
        }
      ],
      "source": [
        "# Print decoder input symbolic shape for verification\n",
        "print('Decoder input symbolic shape:', deInput.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ajZo6ngUjaGx"
      },
      "outputs": [],
      "source": [
        "# Decoder inference model: takes one timestep (one-hot) and previous states, returns distribution + next states\n",
        "\n",
        "deInfInput = Input(shape=(1, len_y), name='decoder_infer_input')\n",
        "deInfHiddenInput = Input(shape=(64,), name='decoder_infer_hidden')\n",
        "deInfCellInput = Input(shape=(64,), name='decoder_infer_cell')\n",
        "\n",
        "deInfOutput, deInfHiddenOutput, deInfCellOutput = deLSTM(deInfInput, initial_state=[deInfHiddenInput, deInfCellInput])\n",
        "deInfOutput2 = deDense(deInfOutput)\n",
        "\n",
        "deInfModel = Model(inputs=[deInfInput, deInfHiddenInput, deInfCellInput],\n",
        "                   outputs=[deInfOutput2, deInfHiddenOutput, deInfCellOutput],\n",
        "                   name='decoder_inference')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkShM_7JAapK"
      },
      "source": [
        "# Test data\n",
        "Tokenize and encode test data using the same vocabulary and padding scheme as training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dg6AXrBGvVYq"
      },
      "outputs": [],
      "source": [
        "# Tokenize test natural language inputs and map to indices (truncate to max_x if longer)\n",
        "test_words_x = [s.split() for s in test_x]\n",
        "test_words_x_idx = [[word2idx_nl.get(w, word2idx_nl['<UNK>']) for w in sent][:max_x] for sent in test_words_x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8oZm6QScBbpw"
      },
      "outputs": [],
      "source": [
        "# Prepare decoder inputs and targets for the test set; use <UNK> when tokens are unseen\n",
        "test_words_y = [['<SOS>'] + s.split() + ['<EOS>'] for s in test_y]\n",
        "test_words_y_idx = [[word2idx_sql.get(w, word2idx_sql['<UNK>']) for w in sent] for sent in test_words_y]\n",
        "\n",
        "test_words_target = [s.split() + ['<EOS>'] for s in test_y]\n",
        "test_words_target_idx = [[word2idx_sql.get(w, word2idx_sql['<UNK>']) for w in sent] for sent in test_words_target]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ljCw3phICZw1"
      },
      "outputs": [],
      "source": [
        "# Pad test sequences to the same lengths used for training\n",
        "\n",
        "test_words_x_idx_pad = [sent + [word2idx_nl['<PAD>']] * (max_x - len(sent)) for sent in test_words_x_idx]\n",
        "\n",
        "test_words_y_idx_pad = [sent + [word2idx_sql['<PAD>']] * (max_y - len(sent)) for sent in test_words_y_idx]\n",
        "\n",
        "test_words_target_idx_pad = [sent + [word2idx_sql['<PAD>']] * (max_y - len(sent)) for sent in test_words_target_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P_zIx18iwT7j"
      },
      "outputs": [],
      "source": [
        "# Vectorized one-hot conversion for test data using `to_categorical`\n",
        "test_words_x_idx_arr = to_categorical(np.array(test_words_x_idx_pad), num_classes=len_x)\n",
        "test_words_y_idx_arr = to_categorical(np.array(test_words_y_idx_pad), num_classes=len_y)\n",
        "test_words_target_idx_arr = to_categorical(np.array(test_words_target_idx_pad), num_classes=len_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Mj5R6FJ3DB7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe6f215-7b1e-461f-f869-4f232e4f876a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test arrays shapes: (186, 23, 741) (186, 52, 397) (186, 52, 397)\n"
          ]
        }
      ],
      "source": [
        "# Quick sanity-check: test arrays shapes (should match training dims)\n",
        "print(\"Test arrays shapes:\", test_words_x_idx_arr.shape, test_words_y_idx_arr.shape, test_words_target_idx_arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ngzgiM58xNMA"
      },
      "outputs": [],
      "source": [
        "# Next: evaluate model on the test set and inspect sample predictions (see following cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0DxsCguYDT_e"
      },
      "outputs": [],
      "source": [
        "test_words_x_idx_arr = test_words_x_idx_arr.reshape(-1, 1, max_x)\n",
        "test_words_y_idx_arr = test_words_y_idx_arr.reshape(-1, 1, max_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b4n-0dxtDbDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ac6aa1-f920-4867-bc31-b75b1f6628e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5035 - loss: 4.3345\n",
            "Test loss: 4.1707, Test accuracy: 51.75%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# assuming test_words_x_idx_pad, test_words_y_idx_pad, len_x, and len_y are still in scope.\n",
        "test_words_x_idx_arr = to_categorical(np.array(test_words_x_idx_pad), num_classes=len_x)\n",
        "test_words_y_idx_arr = to_categorical(np.array(test_words_y_idx_pad), num_classes=len_y)\n",
        "\n",
        "# Evaluate the model on the test set and print loss & accuracy\n",
        "eval_result = model.evaluate(x=[test_words_x_idx_arr, test_words_y_idx_arr], y=test_words_target_idx_arr, verbose=1)\n",
        "print(\"Test loss: {:.4f}, Test accuracy: {:.2f}%\".format(eval_result[0], eval_result[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1z2JIRfla9PX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d62b26-f719-4cdb-d82f-88f9e0a8d580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy of the Encode-Decoder Model is 51.75%\n"
          ]
        }
      ],
      "source": [
        "# formatted test accuracy\n",
        "accuracy = eval_result[1] * 100\n",
        "print(f\"The test accuracy of the Encode-Decoder Model is {round(accuracy, 2)}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}